"""Train ir model"""

import os
import pickle
import zipfile

import numpy as np
import pandas as pd
import requests
from tensorflow.keras import (  # pylint: disable=import-error,no-name-in-module
    layers,
    models,
)
from tensorflow.keras.callbacks import (
    EarlyStopping,  # pylint: disable=import-error,no-name-in-module
)
from tqdm import tqdm

# ðŸ“ URL Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
URL = "https://figshare.com/ndownloader/articles/24593022/versions/1"
DATA_DIR = "train_data_ir"
ARCHIVE_PATH = os.path.join(DATA_DIR, "IR-dataset.zip")
EXTRACT_DIR = DATA_DIR


def download_dataset():
    """Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° IR"""
    os.makedirs(DATA_DIR, exist_ok=True)
    if not os.path.exists(ARCHIVE_PATH):
        print("ðŸ”½ Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÐ¼ IR-dataset...")
        r = requests.get(URL, stream=True, timeout=500)
        with open(ARCHIVE_PATH, "wb") as f:
            for chunk in tqdm(r.iter_content(chunk_size=8192)):
                if chunk:
                    f.write(chunk)
    else:
        print("âœ… ÐÑ€Ñ…Ð¸Ð² ÑƒÐ¶Ðµ ÑÐºÐ°Ñ‡Ð°Ð½.")

    if not os.path.exists(EXTRACT_DIR):
        print("ðŸ“¦ Ð Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ñ‹Ð²Ð°ÐµÐ¼ Ð°Ñ€Ñ…Ð¸Ð²...")
        with zipfile.ZipFile(ARCHIVE_PATH, "r") as zip_ref:
            zip_ref.extractall(DATA_DIR)
    else:
        print("âœ… Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ ÑƒÐ¶Ðµ Ñ€Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ð°Ð½.")


def parse_csv_record(path, min_cm=650, max_cm=4000, step=1.0):
    """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ CSV ÑÐ¿ÐµÐºÑ‚Ñ€Ð° â†’ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ñ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð¼ Ð¿ÐµÑ€Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¸ Ð³Ð¸Ð±ÐºÐ¸Ð¼ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº"""
    try:
        df = pd.read_csv(path, skiprows=1)
    except Exception as e:  # pylint: disable=broad-exception-caught
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ñ„Ð°Ð¹Ð»Ð° {path}: {e}")
        return None

    cm_col = next((c for c in df.columns if "cm" in c.lower()), None)
    t_col = next((c for c in df.columns if "%" in c), None)

    if cm_col is None or t_col is None:
        print(f"âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ cm Ð¸Ð»Ð¸ %T Ð² Ñ„Ð°Ð¹Ð»Ðµ: {path}")
        return None

    df = df[(df[cm_col] >= min_cm) & (df[cm_col] <= max_cm)]
    bins = np.arange(min_cm, max_cm + step, step)
    vec = np.interp(bins, df[cm_col], df[t_col])
    vec = (vec - vec.min()) / (vec.max() - vec.min() + 1e-8)
    return vec.astype(np.float32)


def get_material_name(filename):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð° Ð¸Ð· Ð¸Ð¼ÐµÐ½Ð¸ Ñ„Ð°Ð¹Ð»Ð° (Ð´Ð¾ Ð´ÐµÑ„Ð¸ÑÐ°)"""
    base = os.path.basename(filename)
    name = base.split("-")[0].upper()
    return name


def build_dataset():
    """Build IR dataset"""
    x_indices = []
    y_spectra = []

    files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".csv")]
    print(f"ðŸ” ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(files)} CSV-Ñ„Ð°Ð¹Ð»Ð¾Ð².")

    material_set = set()

    for path in tqdm(files):
        vec = parse_csv_record(path)
        if vec is None:
            continue

        material = get_material_name(path)
        if not material:
            print(f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð´Ð»Ñ Ñ„Ð°Ð¹Ð»Ð°: {path}")
            continue

        material_set.add(material)
        y_spectra.append(vec)

    if not material_set:
        raise ValueError("ÐÐµÑ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°!")

    material_list = sorted(list(material_set))
    with open(os.path.join(DATA_DIR, "material_list.pkl"), "wb") as f:
        pickle.dump(material_list, f)

    x_indices = [material_list.index(get_material_name(path)) for path in files if parse_csv_record(path) is not None]

    x = np.array(x_indices, dtype=np.int32).reshape(-1, 1)  # Ð¸Ð½Ð´ÐµÐºÑ ÐºÐ°Ðº Ð²Ñ…Ð¾Ð´
    y = np.array(y_spectra, dtype=np.float32)
    return x, y, material_list


def build_model(output_dim, n_materials):
    """Build IR model"""
    model = models.Sequential(
        [
            layers.Input(shape=(1,), dtype="int32"),
            layers.Embedding(input_dim=n_materials, output_dim=16),  # embedding Ð´Ð»Ñ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°
            layers.Flatten(),
            layers.Dense(128, activation="relu"),
            layers.Dense(256, activation="relu"),
            layers.Dense(output_dim, activation="linear"),
        ]
    )
    model.compile(optimizer="adam", loss="mse")
    return model


def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð±Ð»Ð¾Ðº"""
    download_dataset()

    x, y, material_list = build_dataset()
    print("âœ… Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ ÑÐ¾Ð±Ñ€Ð°Ð½:", x.shape, y.shape)

    n_materials = len(material_list)
    output_dim = y.shape[1]

    model = build_model(output_dim=output_dim, n_materials=n_materials)
    model.summary()

    early_stop = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
    model.fit(x, y, validation_split=0.1, epochs=50, batch_size=16, callbacks=[early_stop])

    model.save("backend/app/models/ir_spectrum_predictor.keras")
    print("ðŸ’¾ ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð°")


if __name__ == "__main__":
    main()
